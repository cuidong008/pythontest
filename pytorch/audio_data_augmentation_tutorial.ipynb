{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Audio Data Augmentation\n",
        "\n",
        "**Author**: [Moto Hira](moto@meta.com)_\n",
        "\n",
        "``torchaudio`` provides a variety of ways to augment audio data.\n",
        "\n",
        "In this tutorial, we look into a way to apply effects, filters,\n",
        "RIR (room impulse response) and codecs.\n",
        "\n",
        "At the end, we synthesize noisy speech over phone from clean speech.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.functional as F\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torchaudio.__version__)\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparation\n",
        "\n",
        "First, we import the modules and download the audio assets we use in this tutorial.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio\n",
        "\n",
        "# from torchaudio.utils import download_asset\n",
        "\n",
        "# SAMPLE_WAV = download_asset(\"tutorial-assets/steam-train-whistle-daniel_simon.wav\")\n",
        "# SAMPLE_RIR = download_asset(\"tutorial-assets/Lab41-SRI-VOiCES-rm1-impulse-mc01-stu-clo-8000hz.wav\")\n",
        "# SAMPLE_SPEECH = download_asset(\"tutorial-assets/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042-8000hz.wav\")\n",
        "# SAMPLE_NOISE = download_asset(\"tutorial-assets/Lab41-SRI-VOiCES-rm1-babb-mc01-stu-clo-8000hz.wav\")\n",
        "SAMPLE_WAV=\"/workspace/pub/pytorch_tutorial_data/datasets/tutorial-assets/steam-train-whistle-daniel_simon.wav\"\n",
        "SAMPLE_RIR=\"/workspace/pub/pytorch_tutorial_data/datasets/tutorial-assets/Lab41-SRI-VOiCES-rm1-impulse-mc01-stu-clo-8000hz.wav\"\n",
        "SAMPLE_SPEECH=\"/workspace/pub/pytorch_tutorial_data/datasets/tutorial-assets/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042-8000hz.wav\"\n",
        "SAMPLE_NOISE=\"/workspace/pub/pytorch_tutorial_data/datasets/tutorial-assets/Lab41-SRI-VOiCES-rm1-babb-mc01-stu-clo-8000hz.wav\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(SAMPLE_WAV)\n",
        "print(SAMPLE_RIR)\n",
        "print(SAMPLE_SPEECH)\n",
        "print(SAMPLE_NOISE)\n",
        "import torchaudio\n",
        "torchaudio.set_audio_backend(\"sox_io\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Applying effects and filtering\n",
        "\n",
        ":py:class:`torchaudio.io.AudioEffector` allows for directly applying\n",
        "filters and codecs to Tensor objects, in a similar way as ``ffmpeg``\n",
        "command\n",
        "\n",
        "`AudioEffector Usages <./effector_tutorial.html>` explains how to use\n",
        "this class, so for the detail, please refer to the tutorial.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "waveform1, sample_rate = torchaudio.load(SAMPLE_WAV, channels_first=False)\n",
        "\n",
        "# Define effects\n",
        "effect = \",\".join(\n",
        "    [\n",
        "        \"lowpass=frequency=300:poles=1\",  # apply single-pole lowpass filter\n",
        "        \"atempo=0.8\",  # reduce the speed\n",
        "        \"aecho=in_gain=0.8:out_gain=0.9:delays=200:decays=0.3|delays=400:decays=0.3\"\n",
        "        # Applying echo gives some dramatic feeling\n",
        "    ],\n",
        ")\n",
        "\n",
        "\n",
        "# Apply effects\n",
        "def apply_effect(waveform, sample_rate, effect):\n",
        "    effector = torchaudio.io.AudioEffector(effect=effect)\n",
        "    return effector.apply(waveform, sample_rate)\n",
        "\n",
        "\n",
        "waveform2 = apply_effect(waveform1, sample_rate, effect)\n",
        "\n",
        "print(waveform1.shape, sample_rate)\n",
        "print(waveform2.shape, sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the number of frames and number of channels are different from\n",
        "those of the original after the effects are applied. Let’s listen to the\n",
        "audio.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None):\n",
        "    waveform = waveform.numpy()\n",
        "\n",
        "    num_channels, num_frames = waveform.shape\n",
        "    time_axis = torch.arange(0, num_frames) / sample_rate\n",
        "\n",
        "    figure, axes = plt.subplots(num_channels, 1)\n",
        "    if num_channels == 1:\n",
        "        axes = [axes]\n",
        "    for c in range(num_channels):\n",
        "        axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
        "        axes[c].grid(True)\n",
        "        if num_channels > 1:\n",
        "            axes[c].set_ylabel(f\"Channel {c+1}\")\n",
        "        if xlim:\n",
        "            axes[c].set_xlim(xlim)\n",
        "    figure.suptitle(title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_specgram(waveform, sample_rate, title=\"Spectrogram\", xlim=None):\n",
        "    waveform = waveform.numpy()\n",
        "\n",
        "    num_channels, _ = waveform.shape\n",
        "\n",
        "    figure, axes = plt.subplots(num_channels, 1)\n",
        "    if num_channels == 1:\n",
        "        axes = [axes]\n",
        "    for c in range(num_channels):\n",
        "        axes[c].specgram(waveform[c], Fs=sample_rate)\n",
        "        if num_channels > 1:\n",
        "            axes[c].set_ylabel(f\"Channel {c+1}\")\n",
        "        if xlim:\n",
        "            axes[c].set_xlim(xlim)\n",
        "    figure.suptitle(title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Original\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_waveform(waveform1.T, sample_rate, title=\"Original\", xlim=(-0.1, 3.2))\n",
        "plot_specgram(waveform1.T, sample_rate, title=\"Original\", xlim=(0, 3.04))\n",
        "Audio(waveform1.T, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Effects applied\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_waveform(waveform2.T, sample_rate, title=\"Effects Applied\", xlim=(-0.1, 3.2))\n",
        "plot_specgram(waveform2.T, sample_rate, title=\"Effects Applied\", xlim=(0, 3.04))\n",
        "Audio(waveform2.T, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulating room reverberation\n",
        "\n",
        "[Convolution\n",
        "reverb](https://en.wikipedia.org/wiki/Convolution_reverb)_ is a\n",
        "technique that's used to make clean audio sound as though it has been\n",
        "produced in a different environment.\n",
        "\n",
        "Using Room Impulse Response (RIR), for instance, we can make clean speech\n",
        "sound as though it has been uttered in a conference room.\n",
        "\n",
        "For this process, we need RIR data. The following data are from the VOiCES\n",
        "dataset, but you can record your own — just turn on your microphone\n",
        "and clap your hands.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rir_raw, sample_rate = torchaudio.load(SAMPLE_RIR)\n",
        "plot_waveform(rir_raw, sample_rate, title=\"Room Impulse Response (raw)\")\n",
        "plot_specgram(rir_raw, sample_rate, title=\"Room Impulse Response (raw)\")\n",
        "Audio(rir_raw, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we need to clean up the RIR. We extract the main impulse and normalize\n",
        "it by its power.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rir = rir_raw[:, int(sample_rate * 1.01) : int(sample_rate * 1.3)]\n",
        "rir = rir / torch.linalg.vector_norm(rir, ord=2)\n",
        "\n",
        "plot_waveform(rir, sample_rate, title=\"Room Impulse Response\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, using :py:func:`torchaudio.functional.fftconvolve`,\n",
        "we convolve the speech signal with the RIR.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "speech, _ = torchaudio.load(SAMPLE_SPEECH)\n",
        "augmented = F.fftconvolve(speech, rir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Original\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_waveform(speech, sample_rate, title=\"Original\")\n",
        "plot_specgram(speech, sample_rate, title=\"Original\")\n",
        "Audio(speech, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RIR applied\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_waveform(augmented, sample_rate, title=\"RIR Applied\")\n",
        "plot_specgram(augmented, sample_rate, title=\"RIR Applied\")\n",
        "Audio(augmented, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adding background noise\n",
        "\n",
        "To introduce background noise to audio data, we can add a noise Tensor to\n",
        "the Tensor representing the audio data according to some desired\n",
        "signal-to-noise ratio (SNR)\n",
        "[[wikipedia](https://en.wikipedia.org/wiki/Signal-to-noise_ratio)_],\n",
        "which determines the intensity of the audio data relative to that of the noise\n",
        "in the output.\n",
        "\n",
        "$$ \\\\mathrm{SNR} = \\\\frac{P_{signal}}{P_{noise}} $$\n",
        "\n",
        "$$ \\\\mathrm{SNR_{dB}} = 10 \\\\log _{{10}} \\\\mathrm {SNR} $$\n",
        "\n",
        "To add noise to audio data per SNRs, we\n",
        "use :py:func:`torchaudio.functional.add_noise`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "speech, _ = torchaudio.load(SAMPLE_SPEECH)\n",
        "noise, _ = torchaudio.load(SAMPLE_NOISE)\n",
        "noise = noise[:, : speech.shape[1]]\n",
        "\n",
        "snr_dbs = torch.tensor([20, 10, 3])\n",
        "noisy_speeches = F.add_noise(speech, noise, snr_dbs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Background noise\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_waveform(noise, sample_rate, title=\"Background noise\")\n",
        "plot_specgram(noise, sample_rate, title=\"Background noise\")\n",
        "Audio(noise, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SNR 20 dB\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "snr_db, noisy_speech = snr_dbs[0], noisy_speeches[0:1]\n",
        "plot_waveform(noisy_speech, sample_rate, title=f\"SNR: {snr_db} [dB]\")\n",
        "plot_specgram(noisy_speech, sample_rate, title=f\"SNR: {snr_db} [dB]\")\n",
        "Audio(noisy_speech, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SNR 10 dB\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "snr_db, noisy_speech = snr_dbs[1], noisy_speeches[1:2]\n",
        "plot_waveform(noisy_speech, sample_rate, title=f\"SNR: {snr_db} [dB]\")\n",
        "plot_specgram(noisy_speech, sample_rate, title=f\"SNR: {snr_db} [dB]\")\n",
        "Audio(noisy_speech, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SNR 3 dB\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "snr_db, noisy_speech = snr_dbs[2], noisy_speeches[2:3]\n",
        "plot_waveform(noisy_speech, sample_rate, title=f\"SNR: {snr_db} [dB]\")\n",
        "plot_specgram(noisy_speech, sample_rate, title=f\"SNR: {snr_db} [dB]\")\n",
        "Audio(noisy_speech, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Applying codec to Tensor object\n",
        "\n",
        ":py:class:`torchaudio.io.AudioEffector` can also apply codecs to\n",
        "a Tensor object.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "waveform, sample_rate = torchaudio.load(SAMPLE_SPEECH, channels_first=False)\n",
        "\n",
        "\n",
        "def apply_codec(waveform, sample_rate, format, encoder=None):\n",
        "    encoder = torchaudio.io.AudioEffector(format=format, encoder=encoder)\n",
        "    return encoder.apply(waveform, sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Original\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_waveform(waveform.T, sample_rate, title=\"Original\")\n",
        "plot_specgram(waveform.T, sample_rate, title=\"Original\")\n",
        "Audio(waveform.T, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8 bit mu-law\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mulaw = apply_codec(waveform, sample_rate, \"wav\", encoder=\"pcm_mulaw\")\n",
        "plot_waveform(mulaw.T, sample_rate, title=\"8 bit mu-law\")\n",
        "plot_specgram(mulaw.T, sample_rate, title=\"8 bit mu-law\")\n",
        "Audio(mulaw.T, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### G.722\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "g722 = apply_codec(waveform, sample_rate, \"g722\")\n",
        "plot_waveform(g722.T, sample_rate, title=\"G.722\")\n",
        "plot_specgram(g722.T, sample_rate, title=\"G.722\")\n",
        "Audio(g722.T, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vorbis\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vorbis = apply_codec(waveform, sample_rate, \"ogg\", encoder=\"vorbis\")\n",
        "plot_waveform(vorbis.T, sample_rate, title=\"Vorbis\")\n",
        "plot_specgram(vorbis.T, sample_rate, title=\"Vorbis\")\n",
        "Audio(vorbis.T, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulating a phone recoding\n",
        "\n",
        "Combining the previous techniques, we can simulate audio that sounds\n",
        "like a person talking over a phone in a echoey room with people talking\n",
        "in the background.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sample_rate = 16000\n",
        "original_speech, sample_rate = torchaudio.load(SAMPLE_SPEECH)\n",
        "\n",
        "plot_specgram(original_speech, sample_rate, title=\"Original\")\n",
        "\n",
        "# Apply RIR\n",
        "rir_applied = F.fftconvolve(speech, rir)\n",
        "\n",
        "plot_specgram(rir_applied, sample_rate, title=\"RIR Applied\")\n",
        "\n",
        "# Add background noise\n",
        "# Because the noise is recorded in the actual environment, we consider that\n",
        "# the noise contains the acoustic feature of the environment. Therefore, we add\n",
        "# the noise after RIR application.\n",
        "noise, _ = torchaudio.load(SAMPLE_NOISE)\n",
        "noise = noise[:, : rir_applied.shape[1]]\n",
        "\n",
        "snr_db = torch.tensor([8])\n",
        "bg_added = F.add_noise(rir_applied, noise, snr_db)\n",
        "\n",
        "plot_specgram(bg_added, sample_rate, title=\"BG noise added\")\n",
        "\n",
        "# Apply filtering and change sample rate\n",
        "effect = \",\".join(\n",
        "    [\n",
        "        \"lowpass=frequency=4000:poles=1\",\n",
        "        \"compand=attacks=0.02:decays=0.05:points=-60/-60|-30/-10|-20/-8|-5/-8|-2/-8:gain=-8:volume=-7:delay=0.05\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "filtered = apply_effect(bg_added.T, sample_rate, effect)\n",
        "sample_rate2 = 8000\n",
        "\n",
        "plot_specgram(filtered.T, sample_rate2, title=\"Filtered\")\n",
        "\n",
        "# Apply telephony codec\n",
        "codec_applied = apply_codec(filtered, sample_rate2, \"g722\")\n",
        "plot_specgram(codec_applied.T, sample_rate2, title=\"G.722 Codec Applied\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Original speech\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Audio(original_speech, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RIR applied\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Audio(rir_applied, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Background noise added\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Audio(bg_added, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Filtered\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Audio(filtered.T, rate=sample_rate2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Codec applied\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Audio(codec_applied.T, rate=sample_rate2)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
