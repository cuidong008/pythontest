{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Audio Feature Extractions\n",
        "\n",
        "**Author**: [Moto Hira](moto@meta.com)_\n",
        "\n",
        "``torchaudio`` implements feature extractions commonly used in the audio\n",
        "domain. They are available in ``torchaudio.functional`` and\n",
        "``torchaudio.transforms``.\n",
        "\n",
        "``functional`` implements features as standalone functions.\n",
        "They are stateless.\n",
        "\n",
        "``transforms`` implements features as objects,\n",
        "using implementations from ``functional`` and ``torch.nn.Module``.\n",
        "They can be serialized using TorchScript.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.functional as F\n",
        "import torchaudio.transforms as T\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torchaudio.__version__)\n",
        "\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview of audio features\n",
        "\n",
        "The following diagram shows the relationship between common audio features\n",
        "and torchaudio APIs to generate them.\n",
        "\n",
        "<img src=\"https://download.pytorch.org/torchaudio/tutorial-assets/torchaudio_feature_extractions.png\">\n",
        "\n",
        "For the complete list of available features, please refer to the\n",
        "documentation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparation\n",
        "\n",
        "<div class=\"alert alert-info\"><h4>Note</h4><p>When running this tutorial in Google Colab, install the required packages\n",
        "\n",
        "   .. code::\n",
        "\n",
        "      !pip install librosa</p></div>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio\n",
        "from matplotlib.patches import Rectangle\n",
        "from torchaudio.utils import download_asset\n",
        "\n",
        "torch.random.manual_seed(0)\n",
        "\n",
        "# SAMPLE_SPEECH = download_asset(\"tutorial-assets/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\")\n",
        "SAMPLE_SPEECH =\"/workspace/pub/pytorch_tutorial_data/datasets/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n",
        "\n",
        "\n",
        "def plot_waveform(waveform, sr, title=\"Waveform\", ax=None):\n",
        "    waveform = waveform.numpy()\n",
        "\n",
        "    num_channels, num_frames = waveform.shape\n",
        "    time_axis = torch.arange(0, num_frames) / sr\n",
        "\n",
        "    if ax is None:\n",
        "        _, ax = plt.subplots(num_channels, 1)\n",
        "    ax.plot(time_axis, waveform[0], linewidth=1)\n",
        "    ax.grid(True)\n",
        "    ax.set_xlim([0, time_axis[-1]])\n",
        "    ax.set_title(title)\n",
        "\n",
        "\n",
        "def plot_spectrogram(specgram, title=None, ylabel=\"freq_bin\", ax=None):\n",
        "    if ax is None:\n",
        "        _, ax = plt.subplots(1, 1)\n",
        "    if title is not None:\n",
        "        ax.set_title(title)\n",
        "    ax.set_ylabel(ylabel)\n",
        "    ax.imshow(librosa.power_to_db(specgram), origin=\"lower\", aspect=\"auto\", interpolation=\"nearest\")\n",
        "\n",
        "\n",
        "def plot_fbank(fbank, title=None):\n",
        "    fig, axs = plt.subplots(1, 1)\n",
        "    axs.set_title(title or \"Filter bank\")\n",
        "    axs.imshow(fbank, aspect=\"auto\")\n",
        "    axs.set_ylabel(\"frequency bin\")\n",
        "    axs.set_xlabel(\"mel bin\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Spectrogram\n",
        "\n",
        "To get the frequency make-up of an audio signal as it varies with time,\n",
        "you can use :py:func:`torchaudio.transforms.Spectrogram`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Load audio\n",
        "SPEECH_WAVEFORM, SAMPLE_RATE = torchaudio.load(SAMPLE_SPEECH)\n",
        "\n",
        "# Define transform\n",
        "spectrogram = T.Spectrogram(n_fft=512)\n",
        "\n",
        "# Perform transform\n",
        "spec = spectrogram(SPEECH_WAVEFORM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(2, 1)\n",
        "plot_waveform(SPEECH_WAVEFORM, SAMPLE_RATE, title=\"Original waveform\", ax=axs[0])\n",
        "plot_spectrogram(spec[0], title=\"spectrogram\", ax=axs[1])\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Audio(SPEECH_WAVEFORM.numpy(), rate=SAMPLE_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The effect of ``n_fft`` parameter\n",
        "\n",
        "The core of spectrogram computation is (short-term) Fourier transform,\n",
        "and the ``n_fft`` parameter corresponds to the $N$ in the following\n",
        "definition of descrete Fourier transform.\n",
        "\n",
        "$$ X_k = \\\\sum_{n=0}^{N-1} x_n e^{-\\\\frac{2\\\\pi i}{N} nk} $$\n",
        "\n",
        "(For the detail of Fourier transform, please refer to\n",
        "[Wikipedia](https://en.wikipedia.org/wiki/Fast_Fourier_transform)_.\n",
        "\n",
        "The value of ``n_fft`` determines the resolution of frequency axis.\n",
        "However, with the higher ``n_fft`` value, the energy will be distributed\n",
        "among more bins, so when you visualize it, it might look more blurry,\n",
        "even thought they are higher resolution.\n",
        "\n",
        "The following illustrates this;\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>``hop_length`` determines the time axis resolution.\n",
        "   By default, (i.e. ``hop_length=None`` and ``win_length=None``),\n",
        "   the value of ``n_fft // 4`` is used.\n",
        "   Here we use the same ``hop_length`` value across different ``n_fft``\n",
        "   so that they have the same number of elemets in the time axis.</p></div>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_ffts = [32, 128, 512, 2048]\n",
        "hop_length = 64\n",
        "\n",
        "specs = []\n",
        "for n_fft in n_ffts:\n",
        "    spectrogram = T.Spectrogram(n_fft=n_fft, hop_length=hop_length)\n",
        "    spec = spectrogram(SPEECH_WAVEFORM)\n",
        "    specs.append(spec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(len(specs), 1, sharex=True)\n",
        "for i, (spec, n_fft) in enumerate(zip(specs, n_ffts)):\n",
        "    plot_spectrogram(spec[0], ylabel=f\"n_fft={n_fft}\", ax=axs[i])\n",
        "    axs[i].set_xlabel(None)\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When comparing signals, it is desirable to use the same sampling rate,\n",
        "however if you must use the different sampling rate, care must be\n",
        "taken for interpretating the meaning of ``n_fft``.\n",
        "Recall that ``n_fft`` determines the resolution of the frequency\n",
        "axis for a given sampling rate. In other words, what each bin on\n",
        "the frequency axis represents is subject to the sampling rate.\n",
        "\n",
        "As we have seen above, changing the value of ``n_fft`` does not change\n",
        "the coverage of frequency range for the same input signal.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's downsample the audio and apply spectrogram with the same ``n_fft``\n",
        "value.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Downsample to half of the original sample rate\n",
        "speech2 = torchaudio.functional.resample(SPEECH_WAVEFORM, SAMPLE_RATE, SAMPLE_RATE // 2)\n",
        "# Upsample to the original sample rate\n",
        "speech3 = torchaudio.functional.resample(speech2, SAMPLE_RATE // 2, SAMPLE_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Apply the same spectrogram\n",
        "spectrogram = T.Spectrogram(n_fft=512)\n",
        "\n",
        "spec0 = spectrogram(SPEECH_WAVEFORM)\n",
        "spec2 = spectrogram(speech2)\n",
        "spec3 = spectrogram(speech3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Visualize it\n",
        "fig, axs = plt.subplots(3, 1)\n",
        "plot_spectrogram(spec0[0], ylabel=\"Original\", ax=axs[0])\n",
        "axs[0].add_patch(Rectangle((0, 3), 212, 128, edgecolor=\"r\", facecolor=\"none\"))\n",
        "plot_spectrogram(spec2[0], ylabel=\"Downsampled\", ax=axs[1])\n",
        "plot_spectrogram(spec3[0], ylabel=\"Upsampled\", ax=axs[2])\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the above visualization, the second plot (\"Downsampled\") might\n",
        "give the impression that the spectrogram is streched.\n",
        "This is because the meaning of frequency bins is different from\n",
        "the original one.\n",
        "Even though, they have the same number of bins, in the second plot,\n",
        "the frequency is only covered to the half of the original sampling\n",
        "rate.\n",
        "This becomes more clear if we resample the downsampled signal again\n",
        "so that it has the same sample rate as the original.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GriffinLim\n",
        "\n",
        "To recover a waveform from a spectrogram, you can use\n",
        ":py:class:`torchaudio.transforms.GriffinLim`.\n",
        "\n",
        "The same set of parameters used for spectrogram must be used.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define transforms\n",
        "n_fft = 1024\n",
        "spectrogram = T.Spectrogram(n_fft=n_fft)\n",
        "griffin_lim = T.GriffinLim(n_fft=n_fft)\n",
        "\n",
        "# Apply the transforms\n",
        "spec = spectrogram(SPEECH_WAVEFORM)\n",
        "reconstructed_waveform = griffin_lim(spec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "_, axes = plt.subplots(2, 1, sharex=True, sharey=True)\n",
        "plot_waveform(SPEECH_WAVEFORM, SAMPLE_RATE, title=\"Original\", ax=axes[0])\n",
        "plot_waveform(reconstructed_waveform, SAMPLE_RATE, title=\"Reconstructed\", ax=axes[1])\n",
        "Audio(reconstructed_waveform, rate=SAMPLE_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mel Filter Bank\n",
        "\n",
        ":py:func:`torchaudio.functional.melscale_fbanks` generates the filter bank\n",
        "for converting frequency bins to mel-scale bins.\n",
        "\n",
        "Since this function does not require input audio/features, there is no\n",
        "equivalent transform in :py:func:`torchaudio.transforms`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_fft = 256\n",
        "n_mels = 64\n",
        "sample_rate = 6000\n",
        "\n",
        "mel_filters = F.melscale_fbanks(\n",
        "    int(n_fft // 2 + 1),\n",
        "    n_mels=n_mels,\n",
        "    f_min=0.0,\n",
        "    f_max=sample_rate / 2.0,\n",
        "    sample_rate=sample_rate,\n",
        "    norm=\"slaney\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_fbank(mel_filters, \"Mel Filter Bank - torchaudio\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparison against librosa\n",
        "\n",
        "For reference, here is the equivalent way to get the mel filter bank\n",
        "with ``librosa``.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mel_filters_librosa = librosa.filters.mel(\n",
        "    sr=sample_rate,\n",
        "    n_fft=n_fft,\n",
        "    n_mels=n_mels,\n",
        "    fmin=0.0,\n",
        "    fmax=sample_rate / 2.0,\n",
        "    norm=\"slaney\",\n",
        "    htk=True,\n",
        ").T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_fbank(mel_filters_librosa, \"Mel Filter Bank - librosa\")\n",
        "\n",
        "mse = torch.square(mel_filters - mel_filters_librosa).mean().item()\n",
        "print(\"Mean Square Difference: \", mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MelSpectrogram\n",
        "\n",
        "Generating a mel-scale spectrogram involves generating a spectrogram\n",
        "and performing mel-scale conversion. In ``torchaudio``,\n",
        ":py:func:`torchaudio.transforms.MelSpectrogram` provides\n",
        "this functionality.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_fft = 1024\n",
        "win_length = None\n",
        "hop_length = 512\n",
        "n_mels = 128\n",
        "\n",
        "mel_spectrogram = T.MelSpectrogram(\n",
        "    sample_rate=sample_rate,\n",
        "    n_fft=n_fft,\n",
        "    win_length=win_length,\n",
        "    hop_length=hop_length,\n",
        "    center=True,\n",
        "    pad_mode=\"reflect\",\n",
        "    power=2.0,\n",
        "    norm=\"slaney\",\n",
        "    n_mels=n_mels,\n",
        "    mel_scale=\"htk\",\n",
        ")\n",
        "\n",
        "melspec = mel_spectrogram(SPEECH_WAVEFORM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_spectrogram(melspec[0], title=\"MelSpectrogram - torchaudio\", ylabel=\"mel freq\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparison against librosa\n",
        "\n",
        "For reference, here is the equivalent means of generating mel-scale\n",
        "spectrograms with ``librosa``.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "melspec_librosa = librosa.feature.melspectrogram(\n",
        "    y=SPEECH_WAVEFORM.numpy()[0],\n",
        "    sr=sample_rate,\n",
        "    n_fft=n_fft,\n",
        "    hop_length=hop_length,\n",
        "    win_length=win_length,\n",
        "    center=True,\n",
        "    pad_mode=\"reflect\",\n",
        "    power=2.0,\n",
        "    n_mels=n_mels,\n",
        "    norm=\"slaney\",\n",
        "    htk=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_spectrogram(melspec_librosa, title=\"MelSpectrogram - librosa\", ylabel=\"mel freq\")\n",
        "\n",
        "mse = torch.square(melspec - melspec_librosa).mean().item()\n",
        "print(\"Mean Square Difference: \", mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MFCC\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_fft = 2048\n",
        "win_length = None\n",
        "hop_length = 512\n",
        "n_mels = 256\n",
        "n_mfcc = 256\n",
        "\n",
        "mfcc_transform = T.MFCC(\n",
        "    sample_rate=sample_rate,\n",
        "    n_mfcc=n_mfcc,\n",
        "    melkwargs={\n",
        "        \"n_fft\": n_fft,\n",
        "        \"n_mels\": n_mels,\n",
        "        \"hop_length\": hop_length,\n",
        "        \"mel_scale\": \"htk\",\n",
        "    },\n",
        ")\n",
        "\n",
        "mfcc = mfcc_transform(SPEECH_WAVEFORM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_spectrogram(mfcc[0], title=\"MFCC\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparison against librosa\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "melspec = librosa.feature.melspectrogram(\n",
        "    y=SPEECH_WAVEFORM.numpy()[0],\n",
        "    sr=sample_rate,\n",
        "    n_fft=n_fft,\n",
        "    win_length=win_length,\n",
        "    hop_length=hop_length,\n",
        "    n_mels=n_mels,\n",
        "    htk=True,\n",
        "    norm=None,\n",
        ")\n",
        "\n",
        "mfcc_librosa = librosa.feature.mfcc(\n",
        "    S=librosa.core.spectrum.power_to_db(melspec),\n",
        "    n_mfcc=n_mfcc,\n",
        "    dct_type=2,\n",
        "    norm=\"ortho\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_spectrogram(mfcc_librosa, title=\"MFCC (librosa)\")\n",
        "\n",
        "mse = torch.square(mfcc - mfcc_librosa).mean().item()\n",
        "print(\"Mean Square Difference: \", mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LFCC\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_fft = 2048\n",
        "win_length = None\n",
        "hop_length = 512\n",
        "n_lfcc = 256\n",
        "\n",
        "lfcc_transform = T.LFCC(\n",
        "    sample_rate=sample_rate,\n",
        "    n_lfcc=n_lfcc,\n",
        "    speckwargs={\n",
        "        \"n_fft\": n_fft,\n",
        "        \"win_length\": win_length,\n",
        "        \"hop_length\": hop_length,\n",
        "    },\n",
        ")\n",
        "\n",
        "lfcc = lfcc_transform(SPEECH_WAVEFORM)\n",
        "plot_spectrogram(lfcc[0], title=\"LFCC\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pitch\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pitch = F.detect_pitch_frequency(SPEECH_WAVEFORM, SAMPLE_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_pitch(waveform, sr, pitch):\n",
        "    figure, axis = plt.subplots(1, 1)\n",
        "    axis.set_title(\"Pitch Feature\")\n",
        "    axis.grid(True)\n",
        "\n",
        "    end_time = waveform.shape[1] / sr\n",
        "    time_axis = torch.linspace(0, end_time, waveform.shape[1])\n",
        "    axis.plot(time_axis, waveform[0], linewidth=1, color=\"gray\", alpha=0.3)\n",
        "\n",
        "    axis2 = axis.twinx()\n",
        "    time_axis = torch.linspace(0, end_time, pitch.shape[1])\n",
        "    axis2.plot(time_axis, pitch[0], linewidth=2, label=\"Pitch\", color=\"green\")\n",
        "\n",
        "    axis2.legend(loc=0)\n",
        "\n",
        "\n",
        "plot_pitch(SPEECH_WAVEFORM, SAMPLE_RATE, pitch)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
